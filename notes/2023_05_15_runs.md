# Top 50 labels with / without FC conditioning layer


## Without conditioning layer
python run_icd.py \
    --train_file ../data/mimic3/train_50.csv \
    --validation_file ../data/mimic3/test_50.csv \
    --max_length 2500 \
    --chunk_size 2500 \
    --model_name_or_path ../models/roberta-mimic3-full \
    --num_train_epochs 10 \
    --output_dir ../models/roberta-mimic3-full \
    --model_type caml \
    --model_mode laat \
    --per_device_train_batch_size 16 \
    --per_device_eval_batch_size 16 \
    --code_50 \

trained for full 10 epochs!
###
TRAIN METRICS:
metrics: {'acc_macro': 0.0, 'prec_macro': 0.0, 'rec_macro': 0.0, 'f1_macro': 0.0, 'acc_micro': 0.0, 'prec_micro': nan, 'rec_micro': 0.0, 'f1_micro': nan, 'rec_at_8': 0.35470510452225734, 'prec_at_8': 0.24803186213736672, 'f1_at_8': 0.29192889253785415, 'auc_macro': 0.575461178332183, 'auc_micro': 0.6846207576806688}
DEV METRICS:
metrics: {'acc_macro': 0.0, 'prec_macro': 0.0, 'rec_macro': 0.0, 'f1_macro': 0.0, 'acc_micro': 0.0, 'prec_micro': nan, 'rec_micro': 0.0, 'f1_micro': nan, 'rec_at_8': 0.3217389605823927, 'prec_at_8': 0.24117987275882014, 'f1_at_8': 0.27569502734253404, 'auc_macro': 0.5928067779859377, 'auc_micro': 0.6618100923791992}
###

TRAIN metrics:
{'acc_macro': 3.465071301638334e-08, 'prec_macro': 3.736501886808903e-05, 'rec_macro': 3.467214865697606e-08, 'f1_macro': 6.928001026452419e-08, 'acc_micro': 2.1776529256767057e-05, 'prec_micro': 0.3333333333333333, 'rec_micro': 2.1777477732529017e-05, 'f1_micro': 4.355211009973433e-05, 'rec_at_8': 0.3731804688691813, 'prec_at_8': 0.25148772625836846, 'f1_at_8': 0.30048050575323165, 'auc_macro': 0.4230214632633361, 'auc_micro': 0.9983575923872573}
DEV   metrics:
{'acc_macro': 0.0, 'prec_macro': 0.0, 'rec_macro': 0.0, 'f1_macro': 0.0, 'acc_micro': 0.0, 'prec_micro': nan, 'rec_micro': 0.0, 'f1_micro': nan, 'rec_at_8': 0.3255997678555771, 'prec_at_8': 0.2426257952573742, 'f1_at_8': 0.278054729459162, 'auc_macro': 0.4346245501549583, 'auc_micro': 0.9982530516059769}

## Adding conditioning layer:
python run_icd.py \
    --train_file ../data/mimic3/train_50.csv \
    --validation_file ../data/mimic3/test_50.csv \
    --max_length 2500 \
    --chunk_size 2500 \
    --model_name_or_path ../models/roberta-mimic3-full \
    --num_train_epochs 10 \
    --output_dir ../models/roberta-mimic3-full \
    --model_type caml \
    --model_mode laat \
    --per_device_train_batch_size 16 \
    --per_device_eval_batch_size 16 \
    --conditioning

###
TRAIN metrics:
{'acc_macro': 0.0, 'prec_macro': 0.0, 'rec_macro': 0.0, 'f1_macro': 0.0, 'acc_micro': 0.0, 'prec_micro': nan, 'rec_micro': 0.0, 'f1_micro': nan, 'rec_at_8': 0.3548620886241956, 'prec_at_8': 0.24807835358294075, 'f1_at_8': 0.29201425723786717, 'auc_macro': 0.5768474046758186, 'auc_micro': 0.6848968414945127}
DEV metrics:
{'acc_macro': 0.0, 'prec_macro': 0.0, 'rec_macro': 0.0, 'f1_macro': 0.0, 'acc_micro': 0.0, 'prec_micro': nan, 'rec_micro': 0.0, 'f1_micro': nan, 'rec_at_8': 0.3217389605823927, 'prec_at_8': 0.24117987275882014, 'f1_at_8': 0.27569502734253404, 'auc_macro': 0.5928067779859377, 'auc_micro': 0.6618100923791992}
CONDITIONING WEIGHTS
###
trained for full 10 epochs!

TRAIN metrics:
{'acc_macro': 3.465071301638334e-08, 'prec_macro': 3.736501886808903e-05, 'rec_macro': 3.467214865697606e-08, 'f1_macro': 6.928001026452419e-08, 'acc_micro': 2.1776529256767057e-05, 'prec_micro': 0.3333333333333333, 'rec_micro': 2.1777477732529017e-05, 'f1_micro': 4.355211009973433e-05, 'rec_at_8': 0.37319676074327135, 'prec_at_8': 0.25150322340689313, 'f1_at_8': 0.3004968486420894, 'auc_macro': 0.4243808003563715, 'auc_micro': 0.9983581475260545}

DEV   metrics:
{'acc_macro': 0.0, 'prec_macro': 0.0, 'rec_macro': 0.0, 'f1_macro': 0.0, 'acc_micro': 0.0, 'prec_micro': nan, 'rec_micro': 0.0, 'f1_micro': nan, 'rec_at_8': 0.3255997678555771, 'prec_at_8': 0.2426257952573742, 'f1_at_8': 0.278054729459162, 'auc_macro': 0.4346245501549583, 'auc_micro': 0.9982530516059769}

## Adding conditioning layer trained until early stopping after 29 epochs:
python run_icd.py \
    --train_file ../data/mimic3/train_50.csv \
    --validation_file ../data/mimic3/test_50.csv \
    --max_length 2500 \
    --chunk_size 2500 \
    --model_name_or_path ../models/roberta-mimic3-full \
    --num_train_epochs 100 \
    --output_dir ../models/roberta-mimic3-full \
    --model_type caml \
    --model_mode laat \
    --per_device_train_batch_size 16 \
    --per_device_eval_batch_size 16 \
    --conditioning \
    --code_50
    
TRAIN metrics: {'acc_macro': 0.08056932741705762, 'prec_macro': 0.09962412013180577, 'rec_macro': 0.5738934522621725, 'f1_macro': 0.16977620948419544, 'acc_micro': 0.10123287671232876, 'prec_micro': 0.10985533837907631, 'rec_micro': 0.5632744615518631, 'f1_micro': 0.18385371314840152, 'rec_at_8': 0.16596726901975148, 'prec_at_8': 0.11732891148028762, 'f1_at_8': 0.13747279600503343, 'auc_macro': 0.5105173363822683, 'auc_micro': 0.4830165328643941}
DEV METRICS: {'acc_macro': 0.08403214165021873, 'prec_macro': 0.11129689076104912, 'rec_macro': 0.5636938946129726, 'f1_macro': 0.18589106450289306, 'acc_micro': 0.10869805481874448, 'prec_micro': 0.11870373350498874, 'rec_micro': 0.563233750119309, 'f1_micro': 0.19608234062702487, 'rec_at_8': 0.16021052173877676, 'prec_at_8': 0.12001156737998843, 'f1_at_8': 0.13722769596858847, 'auc_macro': 0.5099987324744713, 'auc_micro': 0.48610606528792716}