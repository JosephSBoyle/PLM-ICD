

## With 0.01 weight decay (AdamW default) then with no weight decay
One twentieth of the epochs in the PLM icd paper and one twentieth of the warmup steps.

python run_icd.py \
    --train_file ../data/mimic3/train_full.csv \
    --validation_file ../data/mimic3/dev_full.csv \
    --max_length 3072 \
    --chunk_size 128 \
    --model_name_or_path ../models/RoBERTa-base-PM-M3-Voc-distill-align-hf \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 8 \
    --per_device_eval_batch_size 1 \
    --num_train_epochs 1 \
    --num_warmup_steps 100 \
    --output_dir ../models/roberta-mimic3-full \
    --model_type roberta \
    --model_mode laat \
    --weight_decay 0.01 && \
python run_icd.py \
    --train_file ../data/mimic3/train_full.csv \
    --validation_file ../data/mimic3/dev_full.csv \
    --max_length 3072 \
    --chunk_size 128 \
    --model_name_or_path ../models/RoBERTa-base-PM-M3-Voc-distill-align-hf \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 8 \
    --per_device_eval_batch_size 1 \
    --num_train_epochs 1 \
    --num_warmup_steps 100 \
    --output_dir ../models/roberta-mimic3-full \
    --model_type roberta \
    --model_mode laat
